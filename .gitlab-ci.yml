  variables:
    PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.cache/pip"
    # gitlab docker runner has a mounted volume HOST:/home/cloudadm/.ci-python_packages/ = CONTAINER:/packages
    CACHE_ROOT: "/home/cloudadm/.ci-python_packages"
    CACHE_KEY: "${CI_PIPELINE_ID}"
    CACHE: "${CACHE_ROOT}/${CACHE_KEY}"
    API_HOME: "/home/cloudadm/KitPerlAPIDTK/"
    PACKAGE: "python_packages.tar.gz"
    SCRIPT: "src/sf_datalake/processing/logreg_model.py"
    LAKE_HOME: "/projets/TSF/applicatifs"
    LAKE_TARGET: "${LAKE_HOME}/sf-packages-${CI_COMMIT_REF_SLUG}.tar.gz"
    LAKE_ENTRYPOINT: "${LAKE_HOME}/main.py"
    BRANCH_TO_DEPLOY: "${CI_DEFAULT_BRANCH}"
    #BRANCH_TO_DEPLOY: "feat/enhance_pipeline"


  # Pip's cache doesn't store the python packages
  # https://pip.pypa.io/en/stable/reference/pip_install/#caching
  #
  # If you want to also cache the installed packages, you have to install
  # them in a virtualenv and cache it as well.
  cache:
    paths:
      - .cache/pip
      - venv/

  .python:
    tags:
      - ci
    image: python:3.6.8
    before_script:
      - python -V  # Print out python version for debugging
      - pip install virtualenv
      - virtualenv venv
      - source venv/bin/activate

  .lake:
    tags:
      - datalake
    before_script:
      - cd ${API_HOME}

  ##########################
  #  PIPELINE STARTS HERE  #
  ##########################

  prepare_pipeline_env:
    stage: .pre
    extends: .lake
    script:
      - echo "Prepare cache & conf in ${CACHE}."
      - mkdir -p ${CACHE}
      - mkdir -p ${CACHE}/trace
      - mkdir -p ${CACHE}/log
      - mkdir -p ${CACHE}/bin
      - echo "Prepare maacdo configuration file."
      - envsubst < ${CI_PROJECT_DIR}/.ci/datalake/datalake_config_template.par > ${CACHE}/ci_job.par
      - cat ${CACHE}/ci_job.par
      - echo "Clean older cached data."
      - find ${CACHE_ROOT} -type d -mtime +1 -exec rm -rf {} \; || echo "Please, fix this syntax."

  pylint:
    stage: build
    extends: .python
    script:
    - pylint --rcfile=.pylintrc src/sf_datalake/
    rules:
      - changes:
          - src/sf_datalake/*

  build_package:
    stage: build
    extends: .python
    needs:
      - job: prepare_pipeline_env
    script:
      - pip install -v .
      - venv-pack -o /packages/${CACHE_KEY}/${PACKAGE}

  send_package_to_lake:
    stage: deploy
    extends: .lake
    needs:
      - job: build_package
    script:
      - echo "Remove old package when possible."
      - ./api_maacdo.pl
          -a SuppressionFichier
          -i ${LAKE_TARGET}
          -c ${CACHE}/ci_job.par ||
          echo "Fail removing ${LAKE_TARGET}. Maybe doesn't exist."
      - echo "Send new python package."
      - ./api_maacdo.pl
        -a EnvoyerFichier
        -l ${CACHE}/${PACKAGE}
        -i ${LAKE_TARGET}
        -c ${CACHE}/ci_job.par

  refresh_lake:
    stage: deploy
    extends: .lake
    script:
      - echo "Clean entry point and models on the lake HDFS."
      - ./rmRecursifHdfs.sh sorties_modeles
      - ./api_maacdo.pl
          -a SuppressionFichier
          -i ${LAKE_ENTRYPOINT}
          -c ${CACHE}/ci_job.par ||
          echo "Fail removing file. Maybe doesn't exist."
          - echo "Send new python package."
      - ./api_maacdo.pl
        -a EnvoyerFichier
        -l ${SCRIPT}
        -i ${LAKE_ENTRYPOINT}
        -c ${CACHE}/ci_job.par
    rules:
      - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY

  start_spark_task:
    stage: deploy
    extends: .lake
    needs:
      - job: "refresh_lake"
        optional: true
      - job: "send_package_to_lake"
    script:
      - echo "Launching spark job, getting livy ID."
      - 'idLivy=$(./api_maacdo.pl -a DemarrageTacheSpark -c ${CACHE}/ci_job.par | grep ID |  cut -d: -f2)'
      - echo ${idLivy} | xargs > ${CACHE}/idLivy
    rules:
      - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY

  wait_for_task_finishing:
    stage: deploy
    extends: .lake
    needs: ["start_spark_task"]
    script:
      - export ID_LIVY=$(cat ${CACHE}/idLivy)
      - |
        LAST=""
        while true; do
          TACHE_SPARK=$(./api_maacdo.pl -a EtatTacheSpark   -j ${ID_LIVY} -c ${CACHE}/ci_job.par)
          STATUS=$(echo ${TACHE_SPARK} | awk '{print $NF}' | tr -dc '[:alnum:]\r\n' | tr '[:lower:]' '[:upper:]')
          echo "${STATUS}" > ${CACHE}/status_${ID_LIVY}
          echo "Status of task ${ID_LIVY} is '${STATUS}'"
          [[ ${STATUS} == 'DEAD' ]] && exit 111
          [[ ${STATUS} == 'STARTING' ||  ${STATUS} == 'RUNNING' ]] || break
          LOGS=$(./api_maacdo.pl -a LogsTacheSpark   -j ${ID_LIVY} -c ${CACHE}/ci_job.par)
          CURRENT=$(printf '%s' "${LOGS}" | md5sum)
          if [[ ${LAST} == ${CURRENT} ]]
          then
            echo "No fresh logs, waiting for another 60s."
          else
            echo "New available logs:"
            echo ${LOGS}
            LAST=${CURRENT}
            echo "Waiting for another 60s."
          fi
          sleep 60
        done
    rules:
      - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY

  stop_spark_task:
    stage: deploy
    extends: .lake
    needs: ["start_spark_task"]
    script:
      - export ID_LIVY=$(cat ${CACHE}/idLivy)
      - export STATUS=$(cat ${CACHE}/status_${ID_LIVY})
      - echo "Task ${ID_LIVY} has status ${STATUS}"
      - |
        if [[ ${STATUS} == 'STARTING' ||  ${STATUS} == 'RUNNING' ]]
        then
          ./api_maacdo.pl -a ArretTacheSpark   -j ${ID_LIVY} -c ${CACHE}/ci_job.par
        else
          echo "Nothing to do"
        fi
    rules:
      - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY
        when: manual

  pages:
    stage: deploy
    extends: .python
    script:
      - pwd
      - pip install sphinx sphinx-rtd-theme
      - cd docs/
      - sphinx-apidoc -f -o source/ ../src/
      - make html
      - mv build/html/ ../public/
  #  artifacts:
  #    paths:
  #      - public
    rules:
      - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY
        changes:
        - src/sf_datalake/*

  fetch_yarn_logs:
    stage: .post
    extends: .lake
    needs: ["wait_for_task_finishing"]
    script:
      - export ID_LIVY=$(cat ${CACHE}/idLivy)
      - ./api_maacdo.pl -a YarnLogsTacheSpark  -j ${ID_LIVY} -c ${CACHE}/ci_job.par
      - |
        for file in $(ls ${CACHE}/log/yarnLogsTacheSpark_*)
        do
          echo " --- log file -> ${file} --- "
          tail -n 100 ${file}
          echo " --------------- "
        done
    rules:
    - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY
      when: on_failure

  fetch_model:
    stage: .post
    extends: .lake
    needs: ["wait_for_task_finishing"]
    script:
      - export ID_LIVY=$(cat ${CACHE}/idLivy)
      - echo "Should fetch model"
    rules:
    - if: $CI_COMMIT_BRANCH == $BRANCH_TO_DEPLOY
      when: on_success
  #
  # END of PIPELINE
  #


  print-docker-env:
    stage: .pre
    extends: .python
    script:
      - env | sort
      - pwd
      - whoami
      - ls -al /packages
    when: manual

  print-shell-env:
    stage: .pre
    extends: .lake
    script:
      - env | sort
      - whoami
      - echo "Files on the nubo VM at ${API_HOME}:"
      - ls -al
      - echo "Files on the HDFS at ${LAKE_HOME}:"
      - ./api_maacdo.pl
        -a ListContenuRepertoire
        -i ${LAKE_HOME}
      - echo "Files in ${CACHE}:"
      - ls -al ${CACHE}
    when: manual
