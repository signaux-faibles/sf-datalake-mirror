variables:
  PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.cache/pip"
  # gitlab docker runner has a mounted volume HOST:/home/cloudadm/.ci-python_packages/ = CONTAINER:/packages
  # todo : modifier le point de montage en fonction de la configuration du fichier de config
  CACHE_ROOT: "/home/cloudadm/.ci-python_packages"
  CACHE_KEY: "${CI_PIPELINE_ID}"
  CACHE: "${CACHE_ROOT}/${CACHE_KEY}"
  PACKAGE: "python_packages.tar.gz"
  LAKE_HOME: "/projets/TSF"
  LAKE_MODULES_DIR: "${LAKE_HOME}/applicatifs"
  LAKE_PKG_TARGET: "${LAKE_MODULES_DIR}/sf-packages-${CI_COMMIT_REF_SLUG}.tar.gz"
  LAKE_ENTRYPOINT: "${LAKE_MODULES_DIR}/main.py"
  DEPLOYMENT_TYPE: "null"
  RUN_SPARK_TASK: "no"

### Workflow: defines when branch and merge request pipelines should run.

workflow:
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" || $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
      variables:
        DEPLOYMENT_TYPE: "prod"
        RUN_SPARK_TASK: "yes"
    - if: '$CI_COMMIT_BRANCH == "develop" || $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "develop"'
      variables:
        DEPLOYMENT_TYPE: "small"
        RUN_SPARK_TASK: "yes"
    - if: '$CI_COMMIT_REF_NAME =~ /pipe/' # to keep while there are many improvments on pipelines
      variables:
        DEPLOYMENT_TYPE: "small"
        RUN_SPARK_TASK: "yes"
    - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS'
      when: never
    - if: '$CI_COMMIT_BRANCH != "main" && $CI_COMMIT_BRANCH != "develop"'

# Pip's cache doesn't store the python packages
# https://pip.pypa.io/en/stable/reference/pip_install/#caching
#
# If you want to also cache the installed packages, you have to install
# them in a virtualenv and cache it as well.
cache:
  paths:
    - .cache/pip
    - venv/

.python:
  tags:
    - ci
  image: python:3.6.8
  before_script:
    - python -V  # Print out python version for debugging
    - pip install -U pip
    - pip install virtualenv
    - virtualenv venv
    - source venv/bin/activate

.lake:
  tags:
    - datalake
  before_script:
    - cd ${CACHE}/bin && pwd
  after_script:
    - pwd

##########################
#  PIPELINE STARTS HERE  #
##########################


remove_older_working_directories:
  stage: .pre
  tags:
    - datalake
  script:
    - echo "Cleaning older CI cached data. (Keep only last 10 directory in ${CACHE_ROOT})"
    # on ne garde que les 10 derniers rÃ©pertoires de cache
    - ls -1QAtd ${CACHE_ROOT}/* | tail -n+10 | xargs rm -rf

prepare_new_working_directory:
  stage: .pre
  tags:
    - datalake
  script:
    - echo "Preparing cache & conf in ${CACHE}."
    - "[[ -d ${CACHE} ]] && rm -rf ${CACHE}"
    - mkdir -p ${CACHE}
    - mkdir -p ${CACHE}/trace
    - mkdir -p ${CACHE}/log
    - mkdir -p ${CACHE}/bin
    - mkdir -p ${CACHE}/results
    - echo "Preparing spark configuration inside ${CACHE}/spark_config.json (will be 'null' if no prediction should be run afterwards)."
    - jq --arg REF "${DEPLOYMENT_TYPE}" '.[$REF]' ${CI_PROJECT_DIR}/.ci/datalake/spark_config.json | envsubst > ${CACHE}/spark_config.json
    - cat ${CACHE}/spark_config.json
    - echo "Getting MaacDo version from default or ${DEPLOYMENT_TYPE} or ${CI_COMMIT_REF_SLUG} from .ci/datalake/maacdo_version.json "
    - MAACDO_VERSION=$(jq --raw-output --arg GROUP "${DEPLOYMENT_TYPE}" --arg BRANCH "${CI_COMMIT_REF_SLUG}" '.maacdo | .default + .[$GROUP] + .[$BRANCH] | .version' ${CI_PROJECT_DIR}/.ci/datalake/maacdo_version.json)
    - echo "${MAACDO_VERSION} > ${CACHE}/.maacdo_version"
    - echo "Got MaacDo version as ${MAACDO_VERSION} "
    - git clone --depth 1 --branch $MAACDO_VERSION https://${GIT_MAACDO_USERNAME}:${GIT_MAACDO_TOKEN}@forge.dgfip.finances.rie.gouv.fr/raphaelventura/maac-do.git ${CACHE}/bin

prepare_maacdo_configuration:
  stage: .pre
  extends: .lake
  needs:
    - job: "prepare_new_working_directory"
  script:
    - SPARK_CONFIG=$(jq  --compact-output '.sparkTask' ${CACHE}/spark_config.json)
    - >
     [[ -z ${SPARK_CONFIG} ]]
     && [[ ${DEPLOYMENT_TYPE} != "null" ]]
     && { echo "Error: no config for Spark task excecution was provided."; exit 1; }
    - export SPARK_CONFIG=${SPARK_CONFIG}
    - envsubst < ${CI_PROJECT_DIR}/.ci/datalake/datalake_config_template.par > ${CACHE}/ci_job.par
    - cat ${CACHE}/ci_job.par

pylint:
  stage: test
  extends: .python
  script:
  - pip install pylint
  - pylint --rcfile=.pylintrc src/sf_datalake/
  rules:
    - changes:
      - src/sf_datalake/*

build_package:
  stage: build
  extends: .python
  needs:
    - job: "prepare_new_working_directory"
  script:
    - pip install -v .
    - venv-pack -o /packages/${CACHE_KEY}/${PACKAGE}

send_package_to_lake:
  stage: deploy
  extends: .lake
  needs:
    - job: "build_package"
    - job: "prepare_maacdo_configuration"
  script:
    - echo "Removing old packages archive if it exists."
    - ./api_maacdo.pl
        -a SuppressionFichier
        -i ${LAKE_PKG_TARGET}
        -c ${CACHE}/ci_job.par ||
        echo "Failed removing ${LAKE_PKG_TARGET}. Maybe this file doesn't exist."
    - echo "Sending new python packages archive to ${LAKE_PKG_TARGET}."
    - ./api_maacdo.pl
      -a EnvoyerFichier
      -l ${CACHE}/${PACKAGE}
      -i ${LAKE_PKG_TARGET}
      -c ${CACHE}/ci_job.par

send_entrypoint_to_lake:
  stage: deploy
  extends: .lake
  needs:
    - job: "prepare_maacdo_configuration"
  script:
    - echo "Deleting old configuration when possible"
    - ./api_maacdo.pl
        -a SuppressionFichier
        -i ${LAKE_ENTRYPOINT}
        -c ${CACHE}/ci_job.par ||
        echo "Failed removing file ${LAKE_ENTRYPOINT}. Maybe this file doesn't exist."
    - SCRIPT="${CI_PROJECT_DIR}/src/sf_datalake/__main__.py"
    - echo "Sending new python main script ${SCRIPT} to ${LAKE_ENTRYPOINT}."
    - ./api_maacdo.pl
      -a EnvoyerFichier
      -l ${SCRIPT}
      -i ${LAKE_ENTRYPOINT}
      -c ${CACHE}/ci_job.par
  rules:
    - if: $RUN_SPARK_TASK == "yes"

### Spark jobs handling

start_spark_task:
  stage: deploy
  extends: .lake
  needs:
    - job: "send_entrypoint_to_lake"
    - job: "send_package_to_lake"
  script:
    - echo "Launching spark job, getting livy ID."
    - 'idLivy=$(./api_maacdo.pl -a DemarrageTacheSpark -c ${CACHE}/ci_job.par | grep ID |  cut -d: -f2)'
    - echo "Spark task has started. Livy id is [$idLivy]."
    - echo ${idLivy} | xargs > ${CACHE}/idLivy
    - echo "To get yarns logs even in success case, execute the following from the server running the gitlab runner process:"
    - echo "--> ./api_maacdo.pl -a YarnLogsTacheSpark -j ${idLivy} -c ${CACHE}/ci_job.par"
    - echo "To stop the spark task, execute the following from the server running the gitlab runner process:"
    - echo "--> ./api_maacdo.pl -a ArretTacheSpark -j ${idLivy} -c ${CACHE}/ci_job.par"
  rules:
    - if: $RUN_SPARK_TASK == "yes"

wait_for_task_finishing:
  stage: deploy
  extends: .lake
  needs:
    - job: "start_spark_task"
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - |
      LAST=""
      while true; do
        TACHE_SPARK=$(./api_maacdo.pl -a EtatTacheSpark -j ${ID_LIVY} -c ${CACHE}/ci_job.par)
        STATUS=$(echo ${TACHE_SPARK} | awk '{print $NF}' | tr -dc '[:alnum:]\r\n' | tr '[:lower:]' '[:upper:]')
        echo "${STATUS}" > ${CACHE}/status_${ID_LIVY}
        echo "Status of task ${ID_LIVY} is '${STATUS}'"
        [[ ${STATUS} == 'DEAD' ]] && exit 111
        [[ ${STATUS} == 'STARTING' ||  ${STATUS} == 'RUNNING' ]] || break
        LOGS=$(./api_maacdo.pl -a LogsTacheSpark -j ${ID_LIVY} -c ${CACHE}/ci_job.par)
        CURRENT=$(printf '%s' "${LOGS}" | md5sum)
        if [[ ${LAST} == ${CURRENT} ]]
        then
          echo "No fresh logs, waiting for another 60s."
        else
          echo "New available logs:"
          echo ${LOGS}
          LAST=${CURRENT}
          echo "Waiting for another 60s."
        fi
        sleep 60
      done
  rules:
    - if: $RUN_SPARK_TASK == "yes"

stop_spark_task:
  stage: deploy
  extends: .lake
  needs:
    - job: "start_spark_task"
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - export STATUS=$(cat ${CACHE}/status_${ID_LIVY})
    - echo "Task ${ID_LIVY} has status ${STATUS}."
    - |
      if [[ ${STATUS} == 'STARTING' ||  ${STATUS} == 'RUNNING' ]]
      then
        ./api_maacdo.pl -a ArretTacheSpark -j ${ID_LIVY} -c ${CACHE}/ci_job.par
      else
        echo "Nothing to do."
      fi
  rules:
    - if: $RUN_SPARK_TASK == "yes"
      when: manual
      allow_failure: true

### Post-processing

pages:
  stage: deploy
  extends: .python
  script:
    - pwd
    - pip install sphinx sphinx-rtd-theme
    - cd docs/
    - sphinx-apidoc -f -o source/ ../src/
    - make html
    - mv build/html/ ../public/
#  artifacts:
#    paths:
#      - public
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      changes:
        - src/sf_datalake/*

fetch_yarn_logs:
  stage: .post
  extends: .lake
  needs:
    - job: "wait_for_task_finishing"
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - ./api_maacdo.pl -a YarnLogsTacheSpark -j ${ID_LIVY} -c ${CACHE}/ci_job.par
    - find ${CACHE}/log/ -type f -name 'YarnLogsTacheSpark_*' -execdir cat '{}' + | grep -v 'OnOutOfMemoryError' | grep 'Error' -B 20
  rules:
    - if: $RUN_SPARK_TASK == "yes"
      when: on_failure

fetch_model_outputs:
  stage: .post
  extends: .lake
  needs:
    - job: "wait_for_task_finishing"
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - |
      echo "Parsing ${CACHE}/spark_config.json looking for '--output_directory' argument."
      cat ${CACHE}/spark_config.json
      MODEL_OUTPUT=$(jq --raw-output '[.sparkTask.args as $m | range(0; $m | length; 2) | {($m[.]): $m[(. + 1)]} ] | add | .["--output_directory"]' ${CACHE}/spark_config.json)
      echo "Found '--output_directory'=${MODEL_OUTPUT}"
      [[ -z ${MODEL_OUTPUT} ]] && exit 1
    - bash recup_folder.sh ${MODEL_OUTPUT} ${CACHE}/results ${CACHE}/ci_job.par
    - echo "Gathering fetched outputs into ${CACHE}/results.zip"
    - zip ${CACHE}/results.zip ${CACHE}/results/*
    - rm -r ${CACHE}/results/
  rules:
    - if: $RUN_SPARK_TASK == "yes"

######################
# PIPELINE ENDS HERE #
######################


print-docker-env:
  stage: .pre
  extends: .python
  script:
    - env | sort
    - pwd
    - whoami
    - ls -al /packages
  when: manual

print-shell-env:
  stage: .pre
  extends: .lake
  script:
    - env | sort
    - whoami
    - pwd
    - echo "ID_LIVY = $(cat ${CACHE}/idLivy)"
    - echo "status ID_LIVY = $(cat ${CACHE}/status_${ID_LIVY})"
    - echo "Spark config"
    - cat ${CACHE}/spark_config.json
    - echo "MaacDo version = $(cat ${CACHE}/.maacdo_version"
    - echo "MaacDo configuration"
    - cat ${CACHE}/ci_job.par
  when: manual
