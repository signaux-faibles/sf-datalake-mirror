variables:
  PIP_CACHE_DIR: "${CI_PROJECT_DIR}/.cache/pip"
  # gitlab docker runner has a mounted volume HOST:/home/cloudadm/.ci-python_packages/ = CONTAINER:/packages
  # todo : modifier le point de montage en fonction de la configuration du fichier de config
  CACHE_ROOT: "/home/cloudadm/.ci-python_packages"
  CACHE_KEY: "${CI_PIPELINE_ID}"
  CACHE: "${CACHE_ROOT}/${CACHE_KEY}"
  PACKAGE: "python_packages.tar.gz"
  LAKE_HOME: "/projets/TSF"
  LAKE_MODULES_DIR: "${LAKE_HOME}/applicatifs"
  LAKE_PKG_TARGET: "${LAKE_MODULES_DIR}/sf-packages-${CI_COMMIT_REF_SLUG}.tar.gz"
  LAKE_ENTRYPOINT: "${LAKE_MODULES_DIR}/main.py"
  DEPLOYMENT_TYPE: "null"
  RUN_SPARK_TASK: "no"

workflow:
  rules:
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
      variables:
        DEPLOYMENT_TYPE: "prod"
        RUN_SPARK_TASK: "yes"
    - if: $CI_COMMIT_REF_NAME =~ /^feat/
      variables:
        DEPLOYMENT_TYPE: "feat"
        RUN_SPARK_TASK: "yes"
    - when: always

# Pip's cache doesn't store the python packages
# https://pip.pypa.io/en/stable/reference/pip_install/#caching
#
# If you want to also cache the installed packages, you have to install
# them in a virtualenv and cache it as well.
cache:
  paths:
    - .cache/pip
    - venv/

.python:
  tags:
    - ci
  image: python:3.6.8
  before_script:
    - python -V  # Print out python version for debugging
    - pip install -U pip
    - pip install virtualenv
    - virtualenv venv
    - source venv/bin/activate

.lake:
  tags:
    - datalake
  before_script:
    - cd ${CACHE}/bin && pwd
  after_script:
    - pwd

##########################
#  PIPELINE STARTS HERE  #
##########################


remove_older_working_directories:
  stage: .pre
  tags:
    - datalake
  script:
    - echo "Cleaning older CI cached data."
    # on ne garde que les 5 derniers
    - ls -1QAtd ${CACHE_ROOT}/* | tail -n+10 | xargs rm -rf

prepare_new_working_directory:
  stage: .pre
  tags:
    - datalake
  script:
    - echo "Preparing cache & conf in ${CACHE}."
    - "[[ -d ${CACHE} ]] && rm -rf ${CACHE}"
    - mkdir -p ${CACHE}
    - mkdir -p ${CACHE}/trace
    - mkdir -p ${CACHE}/log
    - mkdir -p ${CACHE}/bin
    - mkdir -p ${CACHE}/results
    - echo "Fetching spark configuration to ${CACHE}/spark_config.json (maybe null)"
    - jq --arg REF "${DEPLOYMENT_TYPE}" '.[$REF]' ${CI_PROJECT_DIR}/.ci/datalake/spark_config.json | envsubst > ${CACHE}/spark_config.json
    - cat ${CACHE}/spark_config.json
    - MAACDO_VERSION=$(jq --raw-output --arg GROUP "${DEPLOYMENT_TYPE}" --arg BRANCH "${CI_COMMIT_REF_SLUG}" '.maacdo | .default + .[$GROUP] + .[$BRANCH] | .version' ${CI_PROJECT_DIR}/.ci/datalake/maacdo_version.json)
    - echo "Getting MaacDo version ${MAACDO_VERSION} "
    - git clone --depth 1 --branch $MAACDO_VERSION https://${GIT_MAACDO_USERNAME}:${GIT_MAACDO_TOKEN}@forge.dgfip.finances.rie.gouv.fr/raphaelventura/maac-do.git ${CACHE}/bin

prepare_maacdo_configuration:
  stage: .pre
  extends: .lake
  needs:
    - job: prepare_new_working_directory
  script:
    - SPARK_CONFIG=$(jq  --compact-output '.sparkTask' ${CACHE}/spark_config.json)
    - >
     [[ -z ${SPARK_CONFIG} ]]
     && [[ ${DEPLOYMENT_TYPE} != "null" ]]
     && { echo "no config for Spark tasks"; exit 1; }
    - export SPARK_CONFIG=${SPARK_CONFIG}
    - envsubst < ${CI_PROJECT_DIR}/.ci/datalake/datalake_config_template.par > ${CACHE}/ci_job.par
    - cat ${CACHE}/ci_job.par

pylint:
  stage: test
  extends: .python
  script:
  - pip install pylint
  - pylint --rcfile=.pylintrc src/sf_datalake/
  rules:
    - changes:
      - src/sf_datalake/*

build_package:
  stage: build
  extends: .python
  needs:
    - job: prepare_new_working_directory
  script:
    - pip install -v .
    - venv-pack -o /packages/${CACHE_KEY}/${PACKAGE}

send_package_to_lake:
  stage: deploy
  extends: .lake
  needs:
    - job: build_package
    - job: prepare_maacdo_configuration
  script:
    - echo "Removing old packages archive if it exists."
    - ./api_maacdo.pl
        -a SuppressionFichier
        -i ${LAKE_PKG_TARGET}
        -c ${CACHE}/ci_job.par ||
        echo "Failed removing ${LAKE_PKG_TARGET}. Maybe this file doesn't exist."
    - echo "Sending new python packages archive to ${LAKE_PKG_TARGET}."
    - ./api_maacdo.pl
      -a EnvoyerFichier
      -l ${CACHE}/${PACKAGE}
      -i ${LAKE_PKG_TARGET}
      -c ${CACHE}/ci_job.par

send_entrypoint_to_lake:
  stage: deploy
  extends: .lake
  needs:
    - job: prepare_maacdo_configuration
  script:
    - ./api_maacdo.pl
        -a SuppressionFichier
        -i ${LAKE_ENTRYPOINT}
        -c ${CACHE}/ci_job.par ||
        echo "Failed removing file ${LAKE_ENTRYPOINT}. Maybe this file doesn't exist."
    - SCRIPT="${CI_PROJECT_DIR}/src/sf_datalake/__main__.py"
    - echo "Sending new python main script ${SCRIPT} to ${LAKE_ENTRYPOINT}."
    - ./api_maacdo.pl
      -a EnvoyerFichier
      -l ${SCRIPT}
      -i ${LAKE_ENTRYPOINT}
      -c ${CACHE}/ci_job.par
  rules:
    - if: $RUN_SPARK_TASK == "yes"

start_spark_task:
  stage: deploy
  extends: .lake
  needs:
    - job: "send_entrypoint_to_lake"
    - job: "send_package_to_lake"
  script:
    - echo "Launching spark job, getting livy ID."
    - 'idLivy=$(./api_maacdo.pl -a DemarrageTacheSpark -c ${CACHE}/ci_job.par | grep ID |  cut -d: -f2)'
    - echo "Spark task has started. Livy id is [$idLivy]."
    - echo ${idLivy} | xargs > ${CACHE}/idLivy
    - echo "to get yarns logs even in success case ->"
    - echo "    ./api_maacdo.pl -a YarnLogsTacheSpark -j ${idLivy} -c ${CACHE}/ci_job.par"
    - echo "to stop Spark task, if job is cancelled ->"
    - echo "    ./api_maacdo.pl -a ArretTacheSpark -j ${idLivy} -c ${CACHE}/ci_job.par"
  rules:
    - if: $RUN_SPARK_TASK == "yes"

wait_for_task_finishing:
  stage: deploy
  extends: .lake
  needs:
    - job: "start_spark_task"
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - |
      LAST=""
      while true; do
        TACHE_SPARK=$(./api_maacdo.pl -a EtatTacheSpark   -j ${ID_LIVY} -c ${CACHE}/ci_job.par)
        STATUS=$(echo ${TACHE_SPARK} | awk '{print $NF}' | tr -dc '[:alnum:]\r\n' | tr '[:lower:]' '[:upper:]')
        echo "${STATUS}" > ${CACHE}/status_${ID_LIVY}
        echo "Status of task ${ID_LIVY} is '${STATUS}'"
        [[ ${STATUS} == 'DEAD' ]] && exit 111
        [[ ${STATUS} == 'STARTING' ||  ${STATUS} == 'RUNNING' ]] || break
        LOGS=$(./api_maacdo.pl -a LogsTacheSpark   -j ${ID_LIVY} -c ${CACHE}/ci_job.par)
        CURRENT=$(printf '%s' "${LOGS}" | md5sum)
        if [[ ${LAST} == ${CURRENT} ]]
        then
          echo "No fresh logs, waiting for another 60s."
        else
          echo "New available logs:"
          echo ${LOGS}
          LAST=${CURRENT}
          echo "Waiting for another 60s."
        fi
        sleep 60
      done
  rules:
    - if: $RUN_SPARK_TASK == "yes"

stop_spark_task:
  stage: deploy
  extends: .lake
  needs:
    - job: "start_spark_task"
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - export STATUS=$(cat ${CACHE}/status_${ID_LIVY})
    - echo "Task ${ID_LIVY} has status ${STATUS}."
    - |
      if [[ ${STATUS} == 'STARTING' ||  ${STATUS} == 'RUNNING' ]]
      then
        ./api_maacdo.pl -a ArretTacheSpark   -j ${ID_LIVY} -c ${CACHE}/ci_job.par
      else
        echo "Nothing to do."
      fi
  when: manual

pages:
  stage: deploy
  extends: .python
  script:
    - pwd
    - pip install sphinx sphinx-rtd-theme
    - cd docs/
    - sphinx-apidoc -f -o source/ ../src/
    - make html
    - mv build/html/ ../public/
#  artifacts:
#    paths:
#      - public
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      changes:
        - src/sf_datalake/*

fetch_yarn_logs:
  stage: .post
  extends: .lake
  needs: ["wait_for_task_finishing"]
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - ./api_maacdo.pl -a YarnLogsTacheSpark  -j ${ID_LIVY} -c ${CACHE}/ci_job.par
    - |
      for file in $(ls ${CACHE}/log/YarnLogsTacheSpark_*)
      do
        echo " --- log file -> ${file} --- "
        tail -n 100 ${file}
        echo " --------------- "
      done
  rules:
    - if: $RUN_SPARK_TASK == "yes"
      when: on_failure

fetch_model_outputs:
  stage: .post
  extends: .lake
  needs: ["wait_for_task_finishing"]
  script:
    - export ID_LIVY=$(cat ${CACHE}/idLivy)
    - |
      MODEL_OUTPUT=$(jq --raw-output '[.sparkTask.args as $m | range(0; $m | length; 2) | {($m[.]): $m[(. + 1)]} ] | add | .["--output_directory"]' ${CACHE}/spark_config.json)
      echo "Fetching results from ${MODEL_OUTPUT}"
      [[ -z ${MODEL_OUTPUT} ]] && exit 1
    - bash recup_folder.sh ${MODEL_OUTPUT} ${CACHE}/results ${CACHE}/ci_job.par
    - |
      for file in ${CACHE}/results/*
      do
        # Gather all csv outputs into a zip and delete the csv files
        # if spark job has succeeded.
        zip ${file}.zip ${file} && rm ${file}
      done
  rules:
    - if: $RUN_SPARK_TASK == "yes"

#
# END of PIPELINE
#

print-docker-env:
  stage: .pre
  extends: .python
  script:
    - env | sort
    - pwd
    - whoami
    - ls -al /packages
  when: manual

print-shell-env:
  stage: .pre
  extends: .lake
  script:
    - env | sort
    - whoami
    - echo "Files on the nubo VM at $(pwd):"
    - ls -al
    - echo "Files on the HDFS at ${LAKE_MODULES_DIR}:"
    - ./api_maacdo.pl
      -a ListContenuRepertoire
      -i ${LAKE_MODULES_DIR}
    - echo "Files in ${CACHE}:"
    - ls -al ${CACHE}
  when: manual
